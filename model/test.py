from charbilstm import charbilstm
from word_lstm import word_lstm
from charcnn import charcnn
import torch.nn as nn
import torch

x_word = torch.tensor([[2, 2973, 10508, 8, 9, 53, 3360, 1739, 4, 139,
                        2211, 1165, 41, 941, 63, 9, 429, 3201, 234, 13405,
                        11, 13174, 301, 6, 3],
                       [2, 256, 4, 10, 1829, 8, 5, 217, 296, 638, 4, 611,
                        13, 1662, 315, 5878, 8, 450, 694, 8, 1961, 5, 424, 6,
                        3]])

x_char = torch.tensor([[[2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 37, 6, 9, 50, 7, 9, 15, 4, 3, 1, 1, 1, 1, 1, 1],
                        [2, 15, 6, 5, 4, 11, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 5, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 17, 7, 11, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 6, 19, 19, 12, 16, 4, 9, 5, 3, 1, 1, 1, 1, 1, 1],
                        [2, 15, 16, 10, 5, 7, 17, 4, 11, 3, 1, 1, 1, 1, 1, 1],
                        [2, 25, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 24, 13, 8, 12, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 37, 6, 9, 15, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 50, 7, 18, 16, 12, 6, 11, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 13, 6, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 6, 12, 24, 6, 21, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 13, 6, 14, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 12, 6, 11, 20, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 18, 11, 4, 10, 4, 9, 15, 4, 3, 1, 1, 1, 1, 1, 1],
                        [2, 6, 17, 7, 9, 20, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 17, 8, 14, 14, 12, 4, 29, 8, 9, 15, 7, 17, 4, 3, 1],
                        [2, 6, 9, 14, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 12, 7, 24, 4, 11, 29, 8, 9, 15, 7, 17, 4, 3, 1, 1],
                        [2, 17, 6, 11, 27, 4, 5, 10, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 22, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
                       [[2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 58, 7, 24, 4, 26, 4, 11, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 25, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 8, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 11, 4, 10, 18, 7, 9, 10, 4, 3, 1, 1, 1, 1, 1, 1],
                        [2, 5, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 5, 13, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 37, 11, 8, 5, 8, 10, 13, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 36, 8, 11, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 14, 4, 15, 8, 10, 8, 7, 9, 3, 1, 1, 1, 1, 1, 1],
                        [2, 25, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 64, 9, 8, 5, 4, 14, 3, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 28, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 18, 8, 12, 7, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 16, 9, 8, 7, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 26, 7, 24, 4, 14, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 5, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 15, 7, 9, 5, 8, 9, 16, 4, 3, 1, 1, 1, 1, 1, 1],
                        [2, 4, 19, 19, 7, 11, 5, 10, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 5, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 11, 4, 26, 8, 26, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 5, 13, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 23, 16, 21, 29, 7, 16, 5, 3, 1, 1, 1, 1, 1, 1, 1],
                        [2, 22, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])
print(x_word.shape)
print(x_char.shape)
# model = charbilstm(hidden_dim=200, vocab_size=100, embedding_dim=100)
model = charcnn(hidden_dim=200, vocab_size=100, embedding_dim=100)
# model = word_lstm(100, 200, 16385, 20, 100, 84)

# hidden_dim_word, embedding_dim_word, vocab_size_word, hidden_dim_char, embedding_dim_char,
#                  vocab_size_char, weights=None, is_bidirectional=False, char_level="bilstm"

y = model(x_char)

# import torch
# from torch import nn
#
# a = torch.randn(1, 3, 5)
# m = nn.Conv1d(3, 2, 3, padding=1, stride=2)
# out = m(a)
# print(a)
# print(out.size())
# print(out)
# print(m)

# kernals = [3, 4, 5, 6]
# cnns = []
# for k in kernals:
#   seq = nn.Sequential(
#                       nn.Conv1d(char_embed_size, output_size // 4, k, padding=0)
#                       # nn.Tanh(),
#                       # nn.MaxPool1d(max_seq_length - k + 1)
#                       )
#   cnns.append(seq)
# self.cnns = nn.ModuleList(cnns)